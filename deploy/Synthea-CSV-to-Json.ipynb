{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "ContainerName = \"claimsfs\"\r\n",
        "LinkedServiceName = \"CoreClaimsDataLake\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\r\n",
        "import uuid\r\n",
        "import json\r\n",
        "\r\n",
        "from pyspark.sql.types import StringType, IntegerType, DateType, DoubleType\r\n",
        "from pyspark.sql.functions import col, rand, concat, current_date, lit, expr, udf, date_add, row_number, collect_list, struct, sum, from_csv\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from urllib.parse import urlparse;\r\n",
        "\r\n",
        "adls_url = json.loads(mssparkutils.credentials.getPropertiesAll(LinkedServiceName))[\"Endpoint\"]\r\n",
        "adls_parsed_url = urlparse(adls_url)\r\n",
        "\r\n",
        "adls_path = f'abfss://{ContainerName}@{adls_parsed_url.netloc}'\r\n",
        "\r\n",
        "input_path = lambda file: (f'{adls_path}/SyntheaInput/{file}')\r\n",
        "output_path = lambda file: (f'{adls_path}/SyntheaOutput/{file}') \r\n",
        "\r\n",
        "random_member_type = udf(lambda: random.choice(['self', 'spouse', 'dependent']), StringType())\r\n",
        "random_adjudicator_type = udf(lambda: random.choice(['Adjudicator', 'Manager']), StringType())\r\n",
        "\r\n",
        "email_from_name = udf(lambda name: f'{name.replace(\" \", \"\").lower()}@contoso.com', StringType())\r\n",
        "\r\n",
        "random_phone_number = udf(lambda: f'({random.randint(0, 999):03})-555-{random.randint(0, 9999):04}', StringType())\r\n",
        "\r\n",
        "make_guid = udf(lambda: str(uuid.uuid4()), StringType())\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adjudicators Transformation\r\n",
        "- Uses the `payers.csv` data source as source since synthia doesn't generate this data\r\n",
        "- Renames existing columns and adds missing column"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_adjudicator = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('payers.csv')) \\\r\n",
        "    .withColumn(\"email\", email_from_name(col(\"NAME\"))) \\\r\n",
        "    .withColumn(\"type\", lit(\"Adjudicator\")) \\\r\n",
        "    .withColumn(\"role\", random_adjudicator_type()) \\\r\n",
        "    .select(\r\n",
        "        col(\"Id\").alias(\"id\"),\\\r\n",
        "        col(\"Id\").alias(\"adjudicatorId\"),\\\r\n",
        "        \"type\",\\\r\n",
        "        col(\"NAME\").alias(\"name\"),\\\r\n",
        "        \"email\",\\\r\n",
        "        \"role\",\r\n",
        "    )\r\n",
        "\r\n",
        "df_adjudicator.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('adjudicator.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Claim Procedures Transformation\r\n",
        "- Load files `procedures.csv`, `careplans.csv`, `immunizations.csv`, `medications.csv`, and `supplies.csv`\r\n",
        "- Select distinct values of the Code and Description fields from each\r\n",
        "- concatinate all lists together"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_procedures(filename):\r\n",
        "    return spark.read.option(\"multiline\", True) \\\r\n",
        "        .option(\"header\", True) \\\r\n",
        "        .csv(input_path(f'{filename}.csv')) \\\r\n",
        "        .withColumn(\"category\", lit(filename)) \\\r\n",
        "        .withColumn(\"type\", lit(\"ClaimProcedure\")) \\\r\n",
        "        .select(\r\n",
        "            col(\"CODE\").alias(\"id\"),\r\n",
        "            col(\"CODE\").alias(\"code\"),\r\n",
        "            col(\"DESCRIPTION\").alias(\"description\"),\r\n",
        "            \"category\"\r\n",
        "        ) \\\r\n",
        "        .distinct()\r\n",
        "\r\n",
        "df_procedures = read_procedures('procedures') \\\r\n",
        "    .union(read_procedures('careplans')) \\\r\n",
        "    .union(read_procedures('immunizations')) \\\r\n",
        "    .union(read_procedures('medications')) \\\r\n",
        "    .union(read_procedures('supplies'))\r\n",
        "\r\n",
        "df_procedures.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('claimprocedure.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Payer Transformation\r\n",
        "- Reads the `payers.csv` data source\r\n",
        "- Renames existing columns and adds missing columns"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_payer = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('payers.csv')) \\\r\n",
        "    .withColumn(\"phoneNumber\", random_phone_number()) \\\r\n",
        "    .withColumn(\"email\", email_from_name(col(\"NAME\"))) \\\r\n",
        "    .withColumn(\"country\", lit(\"US\")) \\\r\n",
        "    .withColumn(\"type\", lit(\"Payer\")) \\\r\n",
        "    .select(\r\n",
        "        col(\"Id\").alias(\"id\"),\r\n",
        "        col(\"Id\").alias(\"payerId\"),\r\n",
        "        col(\"NAME\").alias(\"name\"),\r\n",
        "        \"email\",\r\n",
        "        \"phoneNumber\",\r\n",
        "        col(\"ADDRESS\").alias(\"address\"),\r\n",
        "        col(\"CITY\").alias(\"city\"),\r\n",
        "        col(\"STATE_HEADQUARTERED\").alias(\"state\"),\r\n",
        "        \"country\"\r\n",
        "    )\r\n",
        "\r\n",
        "df_payer.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('payers.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Member Transformation\r\n",
        "- Reads the `patients.csv` data source\r\n",
        "- Renames existing columns and adds missing columns"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_member = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('patients.csv')) \\\r\n",
        "    .withColumn(\"email\", email_from_name(concat(col(\"FIRST\"), col(\"LAST\")))) \\\r\n",
        "    .withColumn(\"phoneNumber\", random_phone_number()) \\\r\n",
        "    .withColumn(\"type\", lit(\"Member\")) \\\r\n",
        "    .withColumn(\"country\", lit(\"US\")) \\\r\n",
        "    .withColumn(\"memberType\", random_member_type()) \\\r\n",
        "    .select(\r\n",
        "        col(\"Id\").alias(\"id\"),\r\n",
        "        col(\"Id\").alias(\"memberId\"), \r\n",
        "        \"memberType\",\r\n",
        "        \"type\",\r\n",
        "        col(\"PREFIX\").alias(\"title\"),\r\n",
        "        col(\"FIRST\").alias(\"firstName\"),\r\n",
        "        col(\"LAST\").alias(\"lastName\"),\r\n",
        "        \"email\",\r\n",
        "        \"phoneNumber\",\r\n",
        "        col(\"ADDRESS\").alias(\"address\"),\r\n",
        "        col(\"CITY\").alias(\"city\"),\r\n",
        "        col(\"STATE\").alias(\"state\"),\r\n",
        "        \"country\",\r\n",
        "        col(\"ZIP\").alias(\"zipCode\"),\r\n",
        "    )\r\n",
        "\r\n",
        "df_member.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('patients.json'))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Member Coverage\r\n",
        "- Gets payer ids from the payer dataframe and randomly assigning it to the member ids and generating a mock data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payer_ids = df_payer.select(\"payerId\").rdd.map(lambda row: row[0]).collect()\r\n",
        "\r\n",
        "random_payer = udf(lambda: random.choice([payer_ids]))\r\n",
        "random_start_offset = udf(lambda: -random.randint(0, 700), IntegerType())\r\n",
        "\r\n",
        "## Make up coverage for members\r\n",
        "## TODO: Potentially pull this from payer_transactions.csv\r\n",
        "df_member_coverage = df_member \\\r\n",
        "    .select(\"memberId\") \\\r\n",
        "    .withColumn(\"type\", lit(\"Coverage\")) \\\r\n",
        "    .withColumn(\"coverageId\", make_guid()) \\\r\n",
        "    .withColumn(\"id\", concat(lit(\"coverage:\"), col(\"coverageId\"))) \\\r\n",
        "    .withColumn(\"startDate\", date_add(current_date(), random_start_offset())) \\\r\n",
        "    .withColumn(\"endDate\", date_add(col(\"startDate\"), 365))\r\n",
        "\r\n",
        "\r\n",
        "df_member_coverage.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('coverage.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provider Transformation\r\n",
        "- Reads the `organization.csv` data source\r\n",
        "- Renames existing columns and adds missing columns"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_provider = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('organizations.csv')) \\\r\n",
        "    .withColumn(\"phoneNumber\", random_phone_number()) \\\r\n",
        "    .withColumn(\"email\", email_from_name(col(\"NAME\"))) \\\r\n",
        "    .withColumn(\"country\", lit(\"US\")) \\\r\n",
        "    .withColumn(\"type\", lit(\"Provider\")) \\\r\n",
        "    .select(\r\n",
        "        col(\"Id\").alias(\"id\"),\r\n",
        "        col(\"Id\").alias(\"providerId\"),\r\n",
        "        col(\"NAME\").alias(\"name\"),\r\n",
        "        \"email\",\r\n",
        "        \"phoneNumber\",\r\n",
        "        col(\"ADDRESS\").alias(\"address\"),\r\n",
        "        col(\"CITY\").alias(\"city\"),\r\n",
        "        col(\"STATE\").alias(\"state\"),\r\n",
        "        \"country\",\r\n",
        "        col(\"ZIP\").alias(\"zip_code\"),\r\n",
        "    )\r\n",
        "\r\n",
        "df_provider.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('providers.json'))\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Claim Header and Claim Detail\r\n",
        "- Reads the `provider.csv`, `claims_transaction.csv`, and `claims.csv`\r\n",
        "- Renames existing columns and adds missing columns\r\n",
        "- Transforms the data by aggregating the `claims_transaction.csv` to get the lineItems\r\n",
        "- Joins everything to a single file to be separated in the Copy Data part of the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claim_window = Window.partitionBy(\"claimId\", \"TYPE\").orderBy(\"claimId\")\r\n",
        "\r\n",
        "# TODO: Load Providers (above providers are actually orgs), map provider to org\r\n",
        "# df_providers = \r\n",
        "\r\n",
        "df_actual_providers = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('providers.csv')) \\\r\n",
        "    .select(\r\n",
        "        col(\"ORGANIZATION\").alias(\"providerId\"),\r\n",
        "        col(\"Id\").alias(\"practitionerId\"),\r\n",
        "    ) \\\r\n",
        "    .join(df_provider.select(\r\n",
        "        \"providerId\",\r\n",
        "        col(\"name\").alias(\"providerName\")\r\n",
        "    ), on=\"providerId\", how=\"inner\") \\\r\n",
        "\r\n",
        "df_transactions = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('claims_transactions.csv')) \\\r\n",
        "    .filter(col(\"TYPE\") == \"CHARGE\") \\\r\n",
        "    .withColumn(\"lineItemNo\", row_number().over(claim_window)) \\\r\n",
        "    .withColumn(\"discount\", lit(0.0)) \\\r\n",
        "    .withColumn(\"amount\", col(\"AMOUNT\").cast(DoubleType())) \\\r\n",
        "    .select(\r\n",
        "        col(\"CLAIMID\").alias(\"claimId\"),\r\n",
        "        \"lineItemNo\",\r\n",
        "        col(\"FROMDATE\").alias(\"serviceDate\"),\r\n",
        "        col(\"PROVIDERID\").alias(\"providerId\"),\r\n",
        "        col(\"PROCEDURECODE\").alias(\"procedureCode\"),\r\n",
        "        col(\"NOTES\").alias(\"description\"),\r\n",
        "        \"amount\",\r\n",
        "        \"discount\"\r\n",
        "     ) \\\r\n",
        "     .groupBy(\"claimId\") \\\r\n",
        "     .agg(\r\n",
        "        collect_list(struct(\r\n",
        "            \"lineItemNo\",\r\n",
        "            \"procedureCode\",\r\n",
        "            \"description\",\r\n",
        "            \"amount\",\r\n",
        "            \"discount\",\r\n",
        "            \"serviceDate\",\r\n",
        "            \"providerId\"\r\n",
        "        )).alias(\"lineItems\"),\r\n",
        "        sum(\"amount\").alias(\"totalAmount\")\r\n",
        "     )\r\n",
        "\r\n",
        "df_claims = spark.read \\\r\n",
        "    .option(\"multiline\", True) \\\r\n",
        "    .option(\"header\", True) \\\r\n",
        "    .csv(input_path('claims.csv')) \\\r\n",
        "    .withColumn(\"adjustmentId\", lit(0)) \\\r\n",
        "    .withColumn(\"claimStatus\", lit(\"Initial\")) \\\r\n",
        "    .withColumn(\"detailId\", concat(lit(\"claim:\"), col(\"Id\"), lit(\":0\"))) \\\r\n",
        "    .withColumn(\"headerId\", concat(lit(\"claim:\"), col(\"Id\"))) \\\r\n",
        "    .select(\r\n",
        "        col(\"Id\").alias(\"claimId\"),\r\n",
        "        col(\"PATIENTID\").alias(\"memberId\"),\r\n",
        "        col(\"SERVICEDATE\").alias(\"filingDate\"),\r\n",
        "        col(\"PROVIDERID\").alias(\"practitionerId\"),\r\n",
        "        \"adjustmentId\",\r\n",
        "        \"claimStatus\",\r\n",
        "        \"headerId\",\r\n",
        "        \"detailId\",\r\n",
        "    ) \\\r\n",
        "    .join(df_transactions, on=\"claimId\", how=\"inner\") \\\r\n",
        "    .join(df_actual_providers.select(\r\n",
        "        \"practitionerId\",\r\n",
        "        \"providerId\",\r\n",
        "        \"providerName\"\r\n",
        "    ), on=\"practitionerId\", how=\"left_outer\") \\\r\n",
        "\r\n",
        "df_claims.write \\\r\n",
        "    .format(\"json\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .json(output_path('claims.json'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}